{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Validate Mongodb Database Setup\n",
    "* Overview of Scrapy Pipelines\n",
    "* Overview of using hash code for quote text\n",
    "* Update Spider Logic to include hash code\n",
    "* Develop Pipeline Logic to write to Mongodb\n",
    "* Run the Pipeline to write to Mongodb\n",
    "* Validate Data in Mongodb Collection\n",
    "* Exercise and Solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Validate Mongodb Database Setup\n",
    "\n",
    "1. Make sure Mongodb is running (Use telnet to validate - `telnet localhost 27017`)\n",
    "2. Launch Mongo shell using `mongosh`.\n",
    "3. We can also use `pymongo` to connect to Mongodb Database using Python.\n",
    "\n",
    "```python\n",
    "import pymongo\n",
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "\n",
    "for db in client.list_databases():\n",
    "    print(db['name'])\n",
    "\n",
    "# We can create new database and then use relevant APIs to deal with collections and documents\n",
    "db = client['quotes_db']\n",
    "\n",
    "# If the database is empty, you will not see any collections\n",
    "for collection in db.list_collections():\n",
    "    print(collection)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Overview of Scrapy Pipelines\n",
    "\n",
    "Here are the details about Scrapy Pipelines.\n",
    "1. We can define pipelines in `pipelines.py`.\n",
    "2. The pipeline class will have the logic to write the data to specified target.\n",
    "3. The logic to process HTML content and write to the target such as database are clearly separated.\n",
    "\n",
    "We will understand how to write the extracted data into Mongo DB database using Scrapy pipelines."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Overview of using hash code for quote text\n",
    "\n",
    "```python\n",
    "import hashlib\n",
    "sha = hashlib.sha256()\n",
    "\n",
    "s = 'Hello World'\n",
    "sha.update(s.encode())\n",
    "\n",
    "sha.hexdigest()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Update Spider Logic to include hash code\n",
    "\n",
    "```python\n",
    "import hashlib\n",
    "import scrapy\n",
    "\n",
    "    \n",
    "class QuoteSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    start_urls = ['https://www.goodreads.com/quotes?page=90']\n",
    "\n",
    "    def parse(self, response):\n",
    "        sha = hashlib.sha256()\n",
    "        for quoteDetails in response.css('.quoteDetails'):\n",
    "            quote_text = quoteDetails.css('.quoteText::text').get()\n",
    "            sha.update(quote_text.encode())\n",
    "            payload = {\n",
    "                'quoteTextHash': sha.hexdigest(),\n",
    "                'quoteText': quote_text,\n",
    "                'authorOrTitle': quoteDetails.css('span.authorOrTitle::text').get(),\n",
    "                'authorOrTitleUrl': quoteDetails.css('a.authorOrTitle::attr(href)').get(),\n",
    "                'authorOrTitleText': quoteDetails.css('a.authorOrTitle::text').get()\n",
    "            }\n",
    "            yield payload\n",
    "\n",
    "        for next_page in response.css('a.next_page'):\n",
    "                yield response.follow(next_page, self.parse)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Develop Pipeline Logic to write to Mongodb\n",
    "\n",
    "Update `pipelines.py`\n",
    "\n",
    "```python\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "class QuotesPipeline:\n",
    "    def __init__(self, mongo_uri, mongo_db, collection_name):\n",
    "        self.mongo_uri = mongo_uri\n",
    "        self.mongo_db = mongo_db\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        mongo_uri = crawler.settings.get('MONGO_URI')\n",
    "        mongo_db = crawler.settings.get('MONGO_DATABASE')\n",
    "        collection_name = crawler.settings.get('MONGO_COLLECTION')\n",
    "        return cls(mongo_uri, mongo_db, collection_name)\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.client = MongoClient(self.mongo_uri)\n",
    "        self.db = self.client[self.mongo_db]\n",
    "        self.collection = self.db[self.collection_name]\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.client.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        # self.collection.insert_one(dict(item))\n",
    "        doc = self.collection.find_one({'quoteTextHash': dict(item)['quoteTextHash']})\n",
    "        if not doc:\n",
    "            self.collection.insert_one(dict(item))\n",
    "        return item\n",
    "```\n",
    "\n",
    "Update `settings.py` with Mongo DB connectivity information and also pipeline details.\n",
    "\n",
    "```python\n",
    "ITEM_PIPELINES = {\n",
    "    'quotes.pipelines.QuotesPipeline': 300,\n",
    "}\n",
    "\n",
    "MONGO_URI = 'mongodb://localhost:27017/'\n",
    "MONGO_DATABASE = 'quotes_db'\n",
    "MONGO_COLLECTION = 'quotes'\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the Pipeline to write to Mongodb\n",
    "\n",
    "Run the pipeline using `scrapy crawl quotes`. It will process the data from the specified urls and load the data into Mongo DB collection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Validate Data in Mongodb Collection\n",
    "\n",
    "1. Launch Mongo Shell\n",
    "2. Switch to quotes_db using `use quotes_db`.\n",
    "3. Check the count in the collection using `db.quotes.countDocuments({})`\n",
    "4. Get first few records using pretty `db.quotes.find({}).pretty()`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Exercise - Include page urls while writing to Mongodb\n",
    "\n",
    "1. Ensure you add the logic related to adding page urls to the `parse` function. The attribute name should be `parseUrl`. It can be populated using `response.url`.\n",
    "2. Make sure data is upserted or merged. If there is no record in mongodb with given quoteTextHash, then the document should be inserted otherwise document should be updated.\n",
    "3. Validate by reviewing the data in the Mongodb collection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Solution - Include page urls while writing to Mongodb\n",
    "\n",
    "1. Update `parse` function in `quotes_spider.py`\n",
    "\n",
    "```python\n",
    "import hashlib\n",
    "import scrapy\n",
    "\n",
    "    \n",
    "class QuoteSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    start_urls = ['https://www.goodreads.com/quotes?page=90']\n",
    "\n",
    "    def parse(self, response):\n",
    "        sha = hashlib.sha256()\n",
    "        for quoteDetails in response.css('.quoteDetails'):\n",
    "            quote_text = quoteDetails.css('.quoteText::text').get()\n",
    "            sha.update(quote_text.encode())\n",
    "            payload = {\n",
    "                'quoteTextHash': sha.hexdigest(),\n",
    "                'pageUrl': response.url,\n",
    "                'quoteText': quote_text,\n",
    "                'authorOrTitle': quoteDetails.css('span.authorOrTitle::text').get(),\n",
    "                'authorOrTitleUrl': quoteDetails.css('a.authorOrTitle::attr(href)').get(),\n",
    "                'authorOrTitleText': quoteDetails.css('a.authorOrTitle::text').get()\n",
    "            }\n",
    "            yield payload\n",
    "\n",
    "        for next_page in response.css('a.next_page'):\n",
    "                yield response.follow(next_page, self.parse)\n",
    "```\n",
    "\n",
    "2. Update `pipelines.py` with required changes to upsert into Mongodb collection\n",
    "\n",
    "```python\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "class QuotesPipeline:\n",
    "    def __init__(self, mongo_uri, mongo_db, collection_name):\n",
    "        self.mongo_uri = mongo_uri\n",
    "        self.mongo_db = mongo_db\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        mongo_uri = crawler.settings.get('MONGO_URI')\n",
    "        mongo_db = crawler.settings.get('MONGO_DATABASE')\n",
    "        collection_name = crawler.settings.get('MONGO_COLLECTION')\n",
    "        return cls(mongo_uri, mongo_db, collection_name)\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.client = MongoClient(self.mongo_uri)\n",
    "        self.db = self.client[self.mongo_db]\n",
    "        self.collection = self.db[self.collection_name]\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.client.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        # self.collection.insert_one(dict(item))\n",
    "        query = {'quoteTextHash': dict(item)['quoteTextHash']}\n",
    "        update = {'$set': dict(item)}\n",
    "        self.collection.update_one(query, update, upsert=True)\n",
    "        return item\n",
    "```\n",
    "\n",
    "3. Run `scrapy crawl quotes` to crawl the data and populate into Mongo collection.\n",
    "4. Run below mongo commands to validate.\n",
    "\n",
    "```mongodb\n",
    "use quotes_db\n",
    "db.quotes.countDocuments({})\n",
    "db.quotes.find({}).pretty()\n",
    "db.quotes.countDocuments({\"pageUrl\": \"https://www.goodreads.com/quotes?page=90\"})\n",
    "db.quotes.find({\"pageUrl\": \"https://www.goodreads.com/quotes?page=90\"}).pretty()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
